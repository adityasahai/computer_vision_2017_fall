function net = proj6_extra_credit_cnn_init()

  rng('default');
  rng(0);

  % constant scalar for the random initial network weights. You shouldn't
  % need to modify this.
  f=1/100; 

  net.layers = {} ;

  net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{f*randn(9,9,1,10, 'single'), zeros(1, 10, 'single')}}, ...
    'stride', 1, ...
    'pad', 0, ...
    'name', 'conv1') ;

  net = insertBnorm(net, size(net.layers,2));

  net.layers{end+1} = struct('type', 'pool', ...
    'method', 'max', ...
    'pool', [2 2], ...
    'stride', 2, ...
    'pad', 0,...
    'name', 'maxPool1') ;

  net.layers{end+1} = struct('type', 'relu') ;

  net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{f*randn(6,6,1,10, 'single'),...
    zeros(1, 10, 'single')}}, ...
    'stride', 2, ...
    'pad', 3, ...
    'name', 'conv2') ;

  net = insertBnorm(net, size(net.layers, 2));

  net.layers{end+1} = struct('type', 'pool', ...
    'method', 'max', ...
    'pool', [3 3], ...
    'stride', 2, ...
    'pad', 0,...
    'name', 'maxPool2') ;

  net.layers{end+1} = struct('type', 'relu') ;

  net.layers{end+1} = struct('type', 'dropout',...
    'rate', 0.5);

  net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{f*randn(6,6,10,250, 'single'),...
    zeros(1, 250, 'single')}}, ...
    'stride', 2, ...
    'pad', 0, ...
    'name', 'fc1') ;

  % Loss layer
  net.layers{end+1} = struct('type', 'softmaxloss') ;

  net = vl_simplenn_tidy(net);

  %You can insert batch normalization layers here
  net = insertBnorm(net, 1);

  net = vl_simplenn_tidy(net) ;

  % Visualize the network
  vl_simplenn_display(net, 'inputSize', [64 64 1 40])
end

% --------------------------------------------------------------------
function net = insertBnorm(net, layer_index)
% --------------------------------------------------------------------
  assert(isfield(net.layers{layer_index}, 'weights'));
  ndim = size(net.layers{layer_index}.weights{1}, 4);
  layer = struct('type', 'bnorm', ...
                 'weights', {{ones(ndim, 1, 'single'), zeros(ndim, 1, 'single')}}, ...
                 'learningRate', [1 1 0.05]) ;
  net.layers{layer_index}.weights{2} = [] ;  % eliminate bias in previous conv layer
  net.layers = horzcat(net.layers(1:layer_index), layer, net.layers(layer_index+1:end)) ;
end


