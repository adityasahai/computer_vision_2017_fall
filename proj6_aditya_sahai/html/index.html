<html>
<head>
<title>Deep Learning Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>Aditya Sahai </h1>
</div>
</div>
<div class="container">

<h2> Project 6 / Deep Learning</h2>
<img src="neural_net2.jpg" alt="">
<p> 	This project is based on building neural networks for image category recognition tasks. We use the MatConvNet toolbox to train deep convolutional networks for scene recognition. The project consisted of two parts.</p>

<ul>
<li><h2>Part 1</h2></li>
<p>Part 1 consisted of building a convolutional network from scratch to recognize screens. As discussed on the Project page we add several elements to the design to make sure that the trained model is good enough to give a reasonable accuracy with test images. I added the following components,</p>
<ul>
	<li><h3>Jitter</h3></li>
	<p>The first change I made was adding jitter. This is basically a process to add variety to the image database used for training the network by using existing images. I made changes such that 50% of the images in each batch are flipped to their mirror images.</p>
	<li><h3>Zero Centering</h3></li>
	<p>For zero centering the images, a mean image was calculated over the entire 1500 image database and substracted from each image. This gives a significant boost of around 10-15% boost the accuracy of the model.</p>
	<li><h3>Regularization</h3></li>
	<p>To prevent overfitting the model to the training data I added, as suggested, dropout regularization with a dropout rate of 0.5. This gives a boost to the accuracy by 8-9%.</p>
	<li><h3>Depth of the network</h3></li>
	<p>To increase the depth of the network I added an extra set of convolution layer, ReLU and maxpool layers. The previous implementation used a maxpool layer which covered a very large area and was loosy. By adding an additional layer, we have a better progression from the image to the last FC layer.</p>
	<li><h3>Batch Normalization</h3></li>
	<p>I added a batch normalization layer after each convolution layer as suggested. This enables us to increase the learning rate and hence the ability to train the network faster than before.</p>
</ul>
<h3>Network</h3>
<p>The following is the code for my network. I have configured the code to run <b>50 epochs</b> with a <b>batch size of 50</b> and learning rate of <b>0.01</b>. I did run the with higher number of epochs and lower learning rates and got better accuracy. The idea was to make sure that the rf size increased gradually to a maximum of 64 (size of image) and the data size decreased gradually to 1.</p>
<pre><code>
	net.layers = {} ;
	
	  net.layers{end+1} = struct('type', 'conv', ...
		'weights', {{f*randn(9,9,1,10, 'single'), zeros(1, 10, 'single')}}, ...
		'stride', 1, ...
		'pad', 0, ...
		'name', 'conv1') ;
	
	  net = insertBnorm(net, size(net.layers,2));
	
	  net.layers{end+1} = struct('type', 'pool', ...
		'method', 'max', ...
		'pool', [2 2], ...
		'stride', 2, ...
		'pad', 0,...
		'name', 'maxPool1') ;
	
	  net.layers{end+1} = struct('type', 'relu') ;
	
	  net.layers{end+1} = struct('type', 'conv', ...
		'weights', {{f*randn(6,6,1,10, 'single'),...
		zeros(1, 10, 'single')}}, ...
		'stride', 2, ...
		'pad', 3, ...
		'name', 'conv2') ;
	
	  net = insertBnorm(net, size(net.layers, 2));
	
	  net.layers{end+1} = struct('type', 'pool', ...
		'method', 'max', ...
		'pool', [3 3], ...
		'stride', 2, ...
		'pad', 0,...
		'name', 'maxPool2') ;
	
	  net.layers{end+1} = struct('type', 'relu') ;
	
	  net.layers{end+1} = struct('type', 'dropout',...
		'rate', 0.5);
	
	  net.layers{end+1} = struct('type', 'conv', ...
		'weights', {{f*randn(6,6,10,15, 'single'),...
		zeros(1, 15, 'single')}}, ...
		'stride', 2, ...
		'pad', 0, ...
		'name', 'fc1') ;

</code></pre>
<h3>Results</h3>
<p>After making all the above mentioned chanages, I get a validation error of <b>0.491333</b> and code for part1 runs and executes in <b>~518 seconds (around 8 minutes)</b>.</p>
<img src="part1.jpg" alt="">
<img src="filter_part1.jpg" alt="">
<p>For a run of <b>1527 seconds</b> with <b>160 epochs</b> and a learning rate of <b>0.01</b>, I get a validation error of 0.42.</p>
<img src="part1_graph_second output.jpg" alt="">
<img src="filter_part1_2.jpg" alt="">

<li><h2>Part 2</h2></li>


<p> Part 2 of this project required us to fine tune an existing network for our purposes. The project page suggests that using existing networks for new tasks is a significantly better strategy than training a new network from scratch.</p>

<p> We take the vgg-f layer and replace the final layer to fit our purposes of 15 scene recognition task. This network works in colored images unlike part1 which was configured to work with B&W images. In this case, we increment the third dimension of the B&W images to 3 to make it work with the network. </p>

<h3>Changes to the network</h3>
<p>The following were the changes done to the existing vgg-f network</p>
<pre><code>
	net.layers{end-1} = struct('type', 'conv', ...
	'weights', {{f*randn(7,7,4096,15, 'single'),...
	zeros(1, 15, 'single')}}, ...
	'stride', 1, ...
	'pad', 3, ...
	'name', 'fc8') ;

net.layers{end} = struct('type', 'softmaxloss') ;

net = insertdropout(net, 18);

net = insertdropout(net, 21);
</code></pre>

<p>I tried adding normalization layers after 5 convolution layers in the vgg-f network but the result was unexpected. Epoch after epoch, the validation error stagnated at around 90% and I had to stop improving the network.</p>

<h3>Additional Settings</h3>

<h3>Results</h3>
<h4>Results 1</h4>
<ul>
	<li>batch size - 50</li>
	<li>learning rate - 0.001</li>
	<li>epochs - 3</li>
</ul>

validation error of <b>0.149333</b> and code for part2 runs and executes in <b>~558 seconds</b>.</p>
<img src="part2_val.jpg" alt="">
<img src="part2_filter.jpg" alt="">

<h4>Results 2</h4>
<ul>
	<li>batch size - 50</li>
	<li>learning rate - 0.001</li>
	<li>epochs - 2</li>
</ul>

validation error of <b>0.154000</b> and code for part2 runs and executes in <b>~380 seconds</b>.</p>
<img src="part2_val4.jpg" alt="">
<img src="part2_filter4.jpg" alt="">

<h4>Results 3</h4>
<ul>
	<li>batch size - 50</li>
	<li>learning rate - 0.001</li>
	<li>epochs - 5</li>
</ul>

validation error of <b>~0.12</b>.</p>
<img src="part2_val3.jpg" alt="">
<li><h2>Extra Credit</h2></li>
<p>For extra credit, I built another Convolutional neural network for the another classification task. As suggested on the project page, I build an NN to classify sketches from the 250 category database on the <a href="http://cybertron.cg.tu-berlin.de/eitz/projects/classifysketch/">Human Object Sketches</a> page.</p>

<p>Since the network needed to trained from scratch, there were several changes that I had to make. Please see below,</p>
<ul>
	<li>Image Categories</li>
	<p>The number of categories for this database was 250 with 80 images for each category. I had to change the number of training images and number of test images to 40 each, each selected at random with replacement from category folders (this might improve the accuracy as some images might repeat).</p>
	<pre><code>
  categories = {'airplane', 'alarm clock', 'angel', 'ant', 'apple', 'arm', 'armchair', 'ashtray', 'axe', 'backpack', 'banana', 
  'barn', 'baseball bat', 'basket', 'bathtub', 'bear (animal)', 'bed', 'bee', 'beer-mug', 'bell', 'bench', 'bicycle', 
  'binoculars', 'blimp', 'book', 'bookshelf', 'boomerang', 'bottle opener', 'bowl', 'brain', 'bread', 'bridge', 'bulldozer',
  'bus', 'bush', 'butterfly', 'cabinet', 'cactus', 'cake', 'calculator', 'camel', 'camera', 'candle', 'cannon', 'canoe',
  'car (sedan)', 'carrot', 'castle', 'cat', 'cell phone', 'chair', 'chandelier', 'church', 'cigarette', 'cloud', 'comb',
  'computer-mouse', 'couch', 'cow', 'crab', 'crane (machine)', 'crocodile', 'crown', 'cup', 'diamond', 'dog', 'dolphin', 
  'donut', 'door', 'door handle', 'dragon', 'duck', 'ear', 'elephant', 'envelope', 'eye', 'eyeglasses','face', 'fan', 'feather',
  'fire hydrant', 'fish', 'flashlight', 'floor lamp', 'flower with stem', 'flying bird','flying saucer', 'foot', 'fork', 'frog',
  'frying-pan', 'giraffe', 'grapes', 'grenade', 'guitar', 'hamburger', 'hammer', 'hand', 'harp', 'hat', 'head', 'head-phones', 
  'hedgehog', 'helicopter', 'helmet', 'horse', 'hot air balloon', 'hot-dog', 'hourglass', 'house', 'human-skeleton', 
  'ice-cream-cone', 'ipod', 'kangaroo', 'key', 'keyboard', 'knife', 'ladder', 'laptop', 'leaf', 'lightbulb', 'lighter', 
  'lion', 'lobster', 'loudspeaker', 'mailbox', 'megaphone', 'mermaid', 'microphone', 'microscope', 'monkey', 'moon', 
  'mosquito', 'motorbike', 'mouse (animal)', 'mouth', 'mug', 'mushroom', 'nose', 'octopus', 'owl', 'palm tree', 
  'panda', 'paper clip', 'parachute', 'parking meter', 'parrot', 'pear', 'pen', 'penguin', 'person sitting', 'computer monitor', 
  'person walking', 'piano', 'pickup truck', 'pig', 'pigeon', 'pineapple', 'pipe (for smoking)', 'pizza', 'potted plant',
  'power outlet', 'present', 'pretzel', 'pumpkin', 'purse', 'rabbit', 'race car', 'radio', 'rainbow', 'revolver', 'rifle', 
  'rollerblades', 'rooster', 'sailboat', 'santa claus', 'satellite', 'satellite dish', 'saxophone', 'scissors', 'scorpion',
  'screwdriver', 'sea turtle', 'seagull', 'shark', 'sheep', 'ship', 'shoe', 'shovel', 'skateboard', 'skull', 'skyscraper', 
  'snail', 'snake', 'snowboard', 'snowman', 'socks', 'space shuttle', 'speed-boat', 'spider', 'sponge bob', 'spoon', 'squirrel', 
  'standing bird', 'stapler', 'strawberry', 'streetlight', 'submarine', 'suitcase', 'sun', 'suv', 'swan', 'sword', 'syringe', 
  't-shirt', 'table', 'tablelamp', 'teacup', 'teapot', 'teddy-bear', 'telephone', 'tennis-racket', 'tent', 'tiger', 'tire',
  'toilet', 'tomato', 'tooth', 'toothbrush', 'tractor', 'traffic light', 'train', 'tree', 'trombone', 'trousers', 'truck', 
  'trumpet', 'tv', 'umbrella', 'van', 'vase', 'violin', 'walkie talkie', 'wheel', 'wheelbarrow', 'windmill', 'wine-bottle', 
  'wineglass', 'wrist-watch', 'zebra'};
	</code></pre>

	<li>Network</li>
	<p>There were slight changes to the network. The changes were made in the last fully-connected layer where the number of filters was increased to 250 (one for each category).</p>
	<pre>
		<code>
  net.layers = {} ;

  net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{f*randn(9,9,1,10, 'single'), zeros(1, 10, 'single')}}, ...
    'stride', 1, ...
    'pad', 0, ...
    'name', 'conv1') ;

  net = insertBnorm(net, size(net.layers,2));

  net.layers{end+1} = struct('type', 'pool', ...
    'method', 'max', ...
    'pool', [2 2], ...
    'stride', 2, ...
    'pad', 0,...
    'name', 'maxPool1') ;

  net.layers{end+1} = struct('type', 'relu') ;

  net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{f*randn(6,6,1,10, 'single'),...
    zeros(1, 10, 'single')}}, ...
    'stride', 2, ...
    'pad', 3, ...
    'name', 'conv2') ;

  net = insertBnorm(net, size(net.layers, 2));

  net.layers{end+1} = struct('type', 'pool', ...
    'method', 'max', ...
    'pool', [3 3], ...
    'stride', 2, ...
    'pad', 0,...
    'name', 'maxPool2') ;

  net.layers{end+1} = struct('type', 'relu') ;

  net.layers{end+1} = struct('type', 'dropout',...
    'rate', 0.5);

  net.layers{end+1} = struct('type', 'conv', ...
    'weights', {{f*randn(6,6,10,<i>250</i>, 'single'),...
    zeros(1, <i>250</i>, 'single')}}, ...
    'stride', 2, ...
    'pad', 0, ...
    'name', 'fc1') ;

		</code>
	</pre>
</ul>
<h4>Results</h4>
<p>Since, there are 250 different categories, each epoch as well as data sampling takes a while. Images are collected with jittering as in part1 (50% of the images are flipped per batch. I changed the batch size to 30 as well. Since, the number of images per category is small, we don't hope to see very high accuracy with the NN model. The original research was able to achieve 56% accuracy. My results are close to that, albeit, with signification training time.</p>
<ul>
	<li>Run 1</li>
	<p>I ran the first run with only 10 epochs and a learning rate of 0.01. The lowest validation error was around 0.63.</p>
	<img src="extra1.jpg" alt="">
	<img src="extra1b.jpg" alt="">
	<li>Run 2</li>
	<p>I ran the second run with 50 epochs and a learning rate of 0.01. The lowest validation error was 0.527100.</p>
	<img src="extra2.jpg" alt="">
	<img src="extra2b.jpg" alt="">
</ul>
</ul>

</body>
</html>


